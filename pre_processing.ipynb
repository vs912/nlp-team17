{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc9c9a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, json, re, random, pathlib\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "from scipy.sparse import save_npz, load_npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef085da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "SEED = 17\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Paths\n",
    "ROOT = pathlib.Path(\".\")\n",
    "RAW  = ROOT / \"data_raw\"      # contains dialogues_*.txt\n",
    "PROC = ROOT / \"data_proc\"     # will be created\n",
    "PROC.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Emotion mapping (DailyDialog)\n",
    "EMO_ID2NAME = {\n",
    "    0: \"no_emotion\",\n",
    "    1: \"anger\",\n",
    "    2: \"disgust\",\n",
    "    3: \"fear\",\n",
    "    4: \"happiness\",\n",
    "    5: \"sadness\",\n",
    "    6: \"surprise\"\n",
    "}\n",
    "EMO_NAME2ID = {v:k for k,v in EMO_ID2NAME.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357a71eb",
   "metadata": {},
   "source": [
    "**Dataset Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9de2f2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 13,118 dialogues.\n"
     ]
    }
   ],
   "source": [
    "text_path = RAW / \"dialogues_text.txt\"\n",
    "emo_path  = RAW / \"dialogues_emotion.txt\"\n",
    "act_path  = RAW / \"dialogues_act.txt\"          \n",
    "\n",
    "texts = text_path.read_text(encoding=\"utf-8\").splitlines()\n",
    "emos  = emo_path.read_text(encoding=\"utf-8\").splitlines()\n",
    "acts  = act_path.read_text(encoding=\"utf-8\").splitlines()\n",
    "\n",
    "assert len(texts) == len(emos) == len(acts), \"Mismatch: texts/emos/acts line counts differ.\"\n",
    "print(f\"Loaded {len(texts):,} dialogues.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0668d4a3",
   "metadata": {},
   "source": [
    "**Flatenning the dialogues to utterance level**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "872b3e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_utt(u: str) -> str:\n",
    "    u = u.strip()\n",
    "    u = re.sub(r\"\\s+\", \" \", u)  # collapse whitespace\n",
    "    return u\n",
    "\n",
    "rows = []\n",
    "bad_align = 0\n",
    "\n",
    "for d_id, (t_line, e_line) in enumerate(zip(texts, emos)):\n",
    "    utts = [clean_utt(u) for u in t_line.split(\"__eou__\") if u.strip()]\n",
    "    e_labels = [int(x) for x in e_line.split() if x != \"\"]\n",
    "    \n",
    "    if len(utts) != len(e_labels):\n",
    "        bad_align += 1\n",
    "        m = min(len(utts), len(e_labels))\n",
    "        utts, e_labels = utts[:m], e_labels[:m]\n",
    "    \n",
    "    for turn_id, (utt, emo_id) in enumerate(zip(utts, e_labels)):\n",
    "        rows.append({\n",
    "            \"dialog_id\": d_id,\n",
    "            \"turn_id\": turn_id,\n",
    "            \"utterance\": utt,\n",
    "            \"emotion_id\": emo_id,\n",
    "            \"emotion\": EMO_ID2NAME.get(emo_id, \"unknown\")\n",
    "        })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f96c029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total utterances: 102,979 | dialogues with length mismatch trimmed: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dialog_id</th>\n",
       "      <th>turn_id</th>\n",
       "      <th>utterance</th>\n",
       "      <th>emotion_id</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>The kitchen stinks .</td>\n",
       "      <td>2</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>I'll throw out the garbage .</td>\n",
       "      <td>0</td>\n",
       "      <td>no_emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>So Dick , how about getting some coffee for to...</td>\n",
       "      <td>4</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Coffee ? I don ’ t honestly like that kind of ...</td>\n",
       "      <td>2</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Come on , you can at least try a little , besi...</td>\n",
       "      <td>0</td>\n",
       "      <td>no_emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>What ’ s wrong with that ? Cigarette is the th...</td>\n",
       "      <td>1</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Not for me , Dick .</td>\n",
       "      <td>0</td>\n",
       "      <td>no_emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Are things still going badly with your housegu...</td>\n",
       "      <td>0</td>\n",
       "      <td>no_emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Getting worse . Now he ’ s eating me out of ho...</td>\n",
       "      <td>1</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Leo , I really think you ’ re beating around t...</td>\n",
       "      <td>0</td>\n",
       "      <td>no_emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dialog_id  turn_id                                          utterance  \\\n",
       "0          0        0                               The kitchen stinks .   \n",
       "1          0        1                       I'll throw out the garbage .   \n",
       "2          1        0  So Dick , how about getting some coffee for to...   \n",
       "3          1        1  Coffee ? I don ’ t honestly like that kind of ...   \n",
       "4          1        2  Come on , you can at least try a little , besi...   \n",
       "5          1        3  What ’ s wrong with that ? Cigarette is the th...   \n",
       "6          1        4                                Not for me , Dick .   \n",
       "7          2        0  Are things still going badly with your housegu...   \n",
       "8          2        1  Getting worse . Now he ’ s eating me out of ho...   \n",
       "9          2        2  Leo , I really think you ’ re beating around t...   \n",
       "\n",
       "   emotion_id     emotion  \n",
       "0           2     disgust  \n",
       "1           0  no_emotion  \n",
       "2           4   happiness  \n",
       "3           2     disgust  \n",
       "4           0  no_emotion  \n",
       "5           1       anger  \n",
       "6           0  no_emotion  \n",
       "7           0  no_emotion  \n",
       "8           1       anger  \n",
       "9           0  no_emotion  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Total utterances: {len(rows):,} | dialogues with length mismatch trimmed: {bad_align}\")\n",
    "df_all = pd.DataFrame(rows)\n",
    "df_all.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ac60f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data_proc/dailydialog_utterances.csv\n",
      "Label map: data_proc/emotion_label_map.json\n"
     ]
    }
   ],
   "source": [
    "master_csv = PROC / \"dailydialog_utterances.csv\"\n",
    "df_all.to_csv(master_csv, index=False, encoding=\"utf-8\")\n",
    "with open(PROC / \"emotion_label_map.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(EMO_ID2NAME, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"Saved:\", master_csv)\n",
    "print(\"Label map:\", PROC / \"emotion_label_map.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "732097d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogs: 13118\n",
      "Utterances: 102979\n",
      "\n",
      "Emotion distribution (counts):\n",
      "emotion\n",
      "no_emotion    85572\n",
      "happiness     12885\n",
      "surprise       1823\n",
      "sadness        1150\n",
      "anger          1022\n",
      "disgust         353\n",
      "fear            174\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Emotion distribution (proportions):\n",
      "emotion\n",
      "no_emotion     83.1%\n",
      "happiness     12.51%\n",
      "surprise       1.77%\n",
      "sadness        1.12%\n",
      "anger          0.99%\n",
      "disgust        0.34%\n",
      "fear           0.17%\n",
      "Name: proportion, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Dialogs:\", df_all[\"dialog_id\"].nunique())\n",
    "print(\"Utterances:\", len(df_all))\n",
    "\n",
    "print(\"\\nEmotion distribution (counts):\")\n",
    "print(df_all[\"emotion\"].value_counts())\n",
    "\n",
    "print(\"\\nEmotion distribution (proportions):\")\n",
    "print((df_all[\"emotion\"].value_counts(normalize=True)*100).round(2).astype(str) + \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ec8873",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da231bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val/Test Dialogues: 10494 / 1311 / 1313\n",
      "Train/Val/Test Utterances: 82687 / 10268 / 10024\n",
      "\n",
      "Wrote split files to: data_proc\n"
     ]
    }
   ],
   "source": [
    "# dialogue-level split (no leakage)\n",
    "dialog_ids = df_all[\"dialog_id\"].unique().tolist()\n",
    "random.shuffle(dialog_ids)\n",
    "\n",
    "n = len(dialog_ids)\n",
    "n_train = int(0.8 * n)\n",
    "n_val   = int(0.1 * n)\n",
    "train_ids = set(dialog_ids[:n_train])\n",
    "val_ids   = set(dialog_ids[n_train:n_train+n_val])\n",
    "test_ids  = set(dialog_ids[n_train+n_val:])\n",
    "\n",
    "def subset(df, ids):\n",
    "    return df[df[\"dialog_id\"].isin(ids)].copy()\n",
    "\n",
    "train_df = subset(df_all, train_ids)\n",
    "val_df   = subset(df_all, val_ids)\n",
    "test_df  = subset(df_all, test_ids)\n",
    "\n",
    "# Attach split column\n",
    "train_df[\"split\"] = \"train\"\n",
    "val_df[\"split\"]   = \"val\"\n",
    "test_df[\"split\"]  = \"test\"\n",
    "\n",
    "print(f\"Train/Val/Test Dialogues: {len(train_ids)} / {len(val_ids)} / {len(test_ids)}\")\n",
    "print(f\"Train/Val/Test Utterances: {len(train_df)} / {len(val_df)} / {len(test_df)}\")\n",
    "\n",
    "# Save\n",
    "train_df.to_csv(PROC/\"train.csv\", index=False, encoding=\"utf-8\")\n",
    "val_df.to_csv(PROC/\"val.csv\", index=False, encoding=\"utf-8\")\n",
    "test_df.to_csv(PROC/\"test.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"\\nWrote split files to:\", PROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd01f0c6",
   "metadata": {},
   "source": [
    "**Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50bf12fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/venusikhakolli/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned training data sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utterance</th>\n",
       "      <th>cleaned_utterance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The kitchen stinks .</td>\n",
       "      <td>kitchen stink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'll throw out the garbage .</td>\n",
       "      <td>ill throw garbage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Are things still going badly with your housegu...</td>\n",
       "      <td>thing still going badly houseguest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Getting worse . Now he ’ s eating me out of ho...</td>\n",
       "      <td>getting worse eating house home tried talking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Leo , I really think you ’ re beating around t...</td>\n",
       "      <td>leo really think beating around bush guy know ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           utterance  \\\n",
       "0                               The kitchen stinks .   \n",
       "1                       I'll throw out the garbage .   \n",
       "7  Are things still going badly with your housegu...   \n",
       "8  Getting worse . Now he ’ s eating me out of ho...   \n",
       "9  Leo , I really think you ’ re beating around t...   \n",
       "\n",
       "                                   cleaned_utterance  \n",
       "0                                      kitchen stink  \n",
       "1                                  ill throw garbage  \n",
       "7                 thing still going badly houseguest  \n",
       "8  getting worse eating house home tried talking ...  \n",
       "9  leo really think beating around bush guy know ...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Advanced Text Cleaning & Normalization\n",
    "# Download necessary NLTK data\n",
    "try:\n",
    "    stopwords.words(\"english\")\n",
    "except LookupError:\n",
    "    nltk.download(\"stopwords\")\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download(\"punkt\")\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "# Contraction mapping\n",
    "CONTRACTION_MAP = {\n",
    "    \"ain't\": \"is not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\", \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\", \"couldn't\": \"could not\", \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\", \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he will\", \"he'll've\": \"he will have\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\", \"how's\": \"how is\", \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\",\n",
    "    \"I'll've\": \"I will have\", \"I'm\": \"I am\", \"I've\": \"I have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\", \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\", \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\", \"needn't've\": \"need not have\", \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\",\n",
    "    \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\", \"so's\": \"so is\", \"that'd\": \"that would\", \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\",\n",
    "    \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\",\n",
    "    \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\",\n",
    "    \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\",\n",
    "    \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "    \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\", \"y'all'd\": \"you all would\", \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\", \"y'all've\": \"you all have\", \"you'd\": \"you would\", \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def advanced_clean_utt(text: str) -> str:\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Expand contractions\n",
    "    text = ' '.join([CONTRACTION_MAP.get(t, t) for t in text.split()])\n",
    "    # Remove punctuation and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Lemmatize and remove stop words\n",
    "    clean_tokens = [\n",
    "        lemmatizer.lemmatize(token) for token in tokens \n",
    "        if token not in stop_words and len(token) > 1\n",
    "    ]\n",
    "    return ' '.join(clean_tokens)\n",
    "\n",
    "# Apply cleaning function\n",
    "for df in [train_df, val_df, test_df]:\n",
    "    df['cleaned_utterance'] = df['utterance'].apply(advanced_clean_utt)\n",
    "\n",
    "print(\"Cleaned training data sample:\")\n",
    "train_df[['utterance', 'cleaned_utterance']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07837fa9",
   "metadata": {},
   "source": [
    "**handling imbalance in training set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95bf0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training set distribution:\n",
      "emotion\n",
      "no_emotion    68524\n",
      "happiness     10513\n",
      "surprise       1455\n",
      "sadness         947\n",
      "anger           832\n",
      "disgust         273\n",
      "fear            143\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Balanced training set distribution:\n",
      "emotion\n",
      "no_emotion    10513\n",
      "happiness     10513\n",
      "surprise       1455\n",
      "sadness         947\n",
      "anger           832\n",
      "disgust         273\n",
      "fear            143\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Saved balanced training set to: data_proc/train_balanced.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Original distribution in training set\n",
    "print(\"Original training set distribution:\")\n",
    "print(train_df['emotion'].value_counts())\n",
    "\n",
    "# Separate majority and minority classes\n",
    "majority_class = train_df[train_df['emotion'] == 'no_emotion']\n",
    "minority_classes = train_df[train_df['emotion'] != 'no_emotion']\n",
    "\n",
    "# Get the size of the next largest class\n",
    "undersample_size = len(train_df[train_df['emotion'] == 'happiness'])\n",
    "\n",
    "# Undersample the majority class\n",
    "majority_undersampled = majority_class.sample(\n",
    "    n=undersample_size, \n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "# Combine with minority classes to create a balanced training set\n",
    "train_df_balanced = pd.concat([majority_undersampled, minority_classes])\n",
    "\n",
    "# Shuffle the balanced dataset\n",
    "train_df_balanced = train_df_balanced.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nBalanced training set distribution:\")\n",
    "print(train_df_balanced['emotion'].value_counts())\n",
    "\n",
    "# Save the balanced training set\n",
    "balanced_train_csv = PROC / \"train_balanced.csv\"\n",
    "train_df_balanced.to_csv(balanced_train_csv, index=False, encoding=\"utf-8\")\n",
    "print(f\"\\nSaved balanced training set to: {balanced_train_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b57947b",
   "metadata": {},
   "source": [
    "**Vectorization for model training**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a707ff63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF vectorization complete.\n",
      "X_train shape: (24676, 5000)\n",
      "X_val shape: (10268, 5000)\n",
      "X_test shape: (10024, 5000)\n",
      "\n",
      "Saved vectorizer to: data_proc/tfidf_vectorizer.joblib\n",
      "Saved data matrices to: data_proc/\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF Vectorization\n",
    "\n",
    "# Initialize the vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),      # Use unigrams and bigrams\n",
    "    max_features=5000,       # Keep top 5k features\n",
    "    sublinear_tf=True        # Apply sublinear TF scaling\n",
    ")\n",
    "\n",
    "# Fit on the balanced training data and transform all splits\n",
    "X_train = tfidf_vectorizer.fit_transform(train_df_balanced['cleaned_utterance'])\n",
    "X_val = tfidf_vectorizer.transform(val_df['cleaned_utterance'])\n",
    "X_test = tfidf_vectorizer.transform(test_df['cleaned_utterance'])\n",
    "\n",
    "# Get the labels\n",
    "y_train = train_df_balanced['emotion_id']\n",
    "y_val = val_df['emotion_id']\n",
    "y_test = test_df['emotion_id']\n",
    "\n",
    "# Save the vectorizer and the processed data\n",
    "joblib.dump(tfidf_vectorizer, PROC / 'tfidf_vectorizer.joblib')\n",
    "\n",
    "# Save the sparse matrices\n",
    "save_npz(PROC / 'X_train.npz', X_train)\n",
    "save_npz(PROC / 'X_val.npz', X_val)\n",
    "save_npz(PROC / 'X_test.npz', X_test)\n",
    "\n",
    "# Save the labels\n",
    "y_train.to_csv(PROC / 'y_train.csv', index=False, header=True)\n",
    "y_val.to_csv(PROC / 'y_val.csv', index=False, header=True)\n",
    "y_test.to_csv(PROC / 'y_test.csv', index=False, header=True)\n",
    "\n",
    "print(\"TF-IDF vectorization complete.\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"\\nSaved vectorizer to: {PROC / 'tfidf_vectorizer.joblib'}\")\n",
    "print(f\"Saved data matrices to: {PROC}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712ac72c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
