{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96ae5318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.executable: /Users/anshureddy/Desktop/NLP Project/nlp-team17/.venv/bin/python\n",
      "python version: 3.12.6 (v3.12.6:a4a2d2b0d85, Sep  6 2024, 16:08:03) [Clang 13.0.0 (clang-1300.0.29.30)]\n",
      "tensorflow version: 2.16.2\n",
      "tensorflow version: 2.16.2\n"
     ]
    }
   ],
   "source": [
    "import sys, json\n",
    "print('sys.executable:', sys.executable)\n",
    "print('python version:', sys.version)\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    print('tensorflow version:', tf.__version__)\n",
    "except Exception as e:\n",
    "    print('tensorflow import error:', repr(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc9c9a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, json, re, random, pathlib\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "from scipy.sparse import save_npz, load_npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef085da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "SEED = 17\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Paths\n",
    "ROOT = pathlib.Path(\".\")\n",
    "RAW  = ROOT / \"data_raw\"      # contains dialogues_*.txt\n",
    "PROC = ROOT / \"data_proc\"     # will be created\n",
    "PROC.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Emotion mapping (DailyDialog)\n",
    "EMO_ID2NAME = {\n",
    "    0: \"no_emotion\",\n",
    "    1: \"anger\",\n",
    "    2: \"disgust\",\n",
    "    3: \"fear\",\n",
    "    4: \"happiness\",\n",
    "    5: \"sadness\",\n",
    "    6: \"surprise\"\n",
    "}\n",
    "EMO_NAME2ID = {v:k for k,v in EMO_ID2NAME.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357a71eb",
   "metadata": {},
   "source": [
    "**Dataset Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9de2f2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 13,118 dialogues.\n"
     ]
    }
   ],
   "source": [
    "text_path = RAW / \"dialogues_text.txt\"\n",
    "emo_path  = RAW / \"dialogues_emotion.txt\"\n",
    "act_path  = RAW / \"dialogues_act.txt\"          \n",
    "\n",
    "texts = text_path.read_text(encoding=\"utf-8\").splitlines()\n",
    "emos  = emo_path.read_text(encoding=\"utf-8\").splitlines()\n",
    "acts  = act_path.read_text(encoding=\"utf-8\").splitlines()\n",
    "\n",
    "assert len(texts) == len(emos) == len(acts), \"Mismatch: texts/emos/acts line counts differ.\"\n",
    "print(f\"Loaded {len(texts):,} dialogues.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0668d4a3",
   "metadata": {},
   "source": [
    "**Flatenning the dialogues to utterance level**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "872b3e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_utt(u: str) -> str:\n",
    "    u = u.strip()\n",
    "    u = re.sub(r\"\\s+\", \" \", u)  # collapse whitespace\n",
    "    return u\n",
    "\n",
    "rows = []\n",
    "bad_align = 0\n",
    "\n",
    "for d_id, (t_line, e_line) in enumerate(zip(texts, emos)):\n",
    "    utts = [clean_utt(u) for u in t_line.split(\"__eou__\") if u.strip()]\n",
    "    e_labels = [int(x) for x in e_line.split() if x != \"\"]\n",
    "    \n",
    "    if len(utts) != len(e_labels):\n",
    "        bad_align += 1\n",
    "        m = min(len(utts), len(e_labels))\n",
    "        utts, e_labels = utts[:m], e_labels[:m]\n",
    "    \n",
    "    for turn_id, (utt, emo_id) in enumerate(zip(utts, e_labels)):\n",
    "        rows.append({\n",
    "            \"dialog_id\": d_id,\n",
    "            \"turn_id\": turn_id,\n",
    "            \"utterance\": utt,\n",
    "            \"emotion_id\": emo_id,\n",
    "            \"emotion\": EMO_ID2NAME.get(emo_id, \"unknown\")\n",
    "        })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f96c029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total utterances: 102,979 | dialogues with length mismatch trimmed: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dialog_id</th>\n",
       "      <th>turn_id</th>\n",
       "      <th>utterance</th>\n",
       "      <th>emotion_id</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>The kitchen stinks .</td>\n",
       "      <td>2</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>I'll throw out the garbage .</td>\n",
       "      <td>0</td>\n",
       "      <td>no_emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>So Dick , how about getting some coffee for to...</td>\n",
       "      <td>4</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Coffee ? I don ’ t honestly like that kind of ...</td>\n",
       "      <td>2</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Come on , you can at least try a little , besi...</td>\n",
       "      <td>0</td>\n",
       "      <td>no_emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>What ’ s wrong with that ? Cigarette is the th...</td>\n",
       "      <td>1</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Not for me , Dick .</td>\n",
       "      <td>0</td>\n",
       "      <td>no_emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Are things still going badly with your housegu...</td>\n",
       "      <td>0</td>\n",
       "      <td>no_emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Getting worse . Now he ’ s eating me out of ho...</td>\n",
       "      <td>1</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Leo , I really think you ’ re beating around t...</td>\n",
       "      <td>0</td>\n",
       "      <td>no_emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dialog_id  turn_id                                          utterance  \\\n",
       "0          0        0                               The kitchen stinks .   \n",
       "1          0        1                       I'll throw out the garbage .   \n",
       "2          1        0  So Dick , how about getting some coffee for to...   \n",
       "3          1        1  Coffee ? I don ’ t honestly like that kind of ...   \n",
       "4          1        2  Come on , you can at least try a little , besi...   \n",
       "5          1        3  What ’ s wrong with that ? Cigarette is the th...   \n",
       "6          1        4                                Not for me , Dick .   \n",
       "7          2        0  Are things still going badly with your housegu...   \n",
       "8          2        1  Getting worse . Now he ’ s eating me out of ho...   \n",
       "9          2        2  Leo , I really think you ’ re beating around t...   \n",
       "\n",
       "   emotion_id     emotion  \n",
       "0           2     disgust  \n",
       "1           0  no_emotion  \n",
       "2           4   happiness  \n",
       "3           2     disgust  \n",
       "4           0  no_emotion  \n",
       "5           1       anger  \n",
       "6           0  no_emotion  \n",
       "7           0  no_emotion  \n",
       "8           1       anger  \n",
       "9           0  no_emotion  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Total utterances: {len(rows):,} | dialogues with length mismatch trimmed: {bad_align}\")\n",
    "df_all = pd.DataFrame(rows)\n",
    "df_all.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ac60f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data_proc/dailydialog_utterances.csv\n",
      "Label map: data_proc/emotion_label_map.json\n"
     ]
    }
   ],
   "source": [
    "master_csv = PROC / \"dailydialog_utterances.csv\"\n",
    "df_all.to_csv(master_csv, index=False, encoding=\"utf-8\")\n",
    "with open(PROC / \"emotion_label_map.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(EMO_ID2NAME, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"Saved:\", master_csv)\n",
    "print(\"Label map:\", PROC / \"emotion_label_map.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "732097d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogs: 13118\n",
      "Utterances: 102979\n",
      "\n",
      "Emotion distribution (counts):\n",
      "emotion\n",
      "no_emotion    85572\n",
      "happiness     12885\n",
      "surprise       1823\n",
      "sadness        1150\n",
      "anger          1022\n",
      "disgust         353\n",
      "fear            174\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Emotion distribution (proportions):\n",
      "emotion\n",
      "no_emotion     83.1%\n",
      "happiness     12.51%\n",
      "surprise       1.77%\n",
      "sadness        1.12%\n",
      "anger          0.99%\n",
      "disgust        0.34%\n",
      "fear           0.17%\n",
      "Name: proportion, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Dialogs:\", df_all[\"dialog_id\"].nunique())\n",
    "print(\"Utterances:\", len(df_all))\n",
    "\n",
    "print(\"\\nEmotion distribution (counts):\")\n",
    "print(df_all[\"emotion\"].value_counts())\n",
    "\n",
    "print(\"\\nEmotion distribution (proportions):\")\n",
    "print((df_all[\"emotion\"].value_counts(normalize=True)*100).round(2).astype(str) + \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ec8873",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da231bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val/Test Dialogues: 10494 / 1311 / 1313\n",
      "Train/Val/Test Utterances: 82687 / 10268 / 10024\n",
      "\n",
      "Wrote split files to: data_proc\n"
     ]
    }
   ],
   "source": [
    "# dialogue-level split (no leakage)\n",
    "dialog_ids = df_all[\"dialog_id\"].unique().tolist()\n",
    "random.shuffle(dialog_ids)\n",
    "\n",
    "n = len(dialog_ids)\n",
    "n_train = int(0.8 * n)\n",
    "n_val   = int(0.1 * n)\n",
    "train_ids = set(dialog_ids[:n_train])\n",
    "val_ids   = set(dialog_ids[n_train:n_train+n_val])\n",
    "test_ids  = set(dialog_ids[n_train+n_val:])\n",
    "\n",
    "def subset(df, ids):\n",
    "    return df[df[\"dialog_id\"].isin(ids)].copy()\n",
    "\n",
    "train_df = subset(df_all, train_ids)\n",
    "val_df   = subset(df_all, val_ids)\n",
    "test_df  = subset(df_all, test_ids)\n",
    "\n",
    "# Attach split column\n",
    "train_df[\"split\"] = \"train\"\n",
    "val_df[\"split\"]   = \"val\"\n",
    "test_df[\"split\"]  = \"test\"\n",
    "\n",
    "print(f\"Train/Val/Test Dialogues: {len(train_ids)} / {len(val_ids)} / {len(test_ids)}\")\n",
    "print(f\"Train/Val/Test Utterances: {len(train_df)} / {len(val_df)} / {len(test_df)}\")\n",
    "\n",
    "# Save\n",
    "train_df.to_csv(PROC/\"train.csv\", index=False, encoding=\"utf-8\")\n",
    "val_df.to_csv(PROC/\"val.csv\", index=False, encoding=\"utf-8\")\n",
    "test_df.to_csv(PROC/\"test.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"\\nWrote split files to:\", PROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd01f0c6",
   "metadata": {},
   "source": [
    "**Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50bf12fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/anshureddy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned training data sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utterance</th>\n",
       "      <th>cleaned_utterance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The kitchen stinks .</td>\n",
       "      <td>kitchen stink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'll throw out the garbage .</td>\n",
       "      <td>ill throw garbage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Are things still going badly with your housegu...</td>\n",
       "      <td>thing still going badly houseguest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Getting worse . Now he ’ s eating me out of ho...</td>\n",
       "      <td>getting worse eating house home tried talking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Leo , I really think you ’ re beating around t...</td>\n",
       "      <td>leo really think beating around bush guy know ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           utterance  \\\n",
       "0                               The kitchen stinks .   \n",
       "1                       I'll throw out the garbage .   \n",
       "7  Are things still going badly with your housegu...   \n",
       "8  Getting worse . Now he ’ s eating me out of ho...   \n",
       "9  Leo , I really think you ’ re beating around t...   \n",
       "\n",
       "                                   cleaned_utterance  \n",
       "0                                      kitchen stink  \n",
       "1                                  ill throw garbage  \n",
       "7                 thing still going badly houseguest  \n",
       "8  getting worse eating house home tried talking ...  \n",
       "9  leo really think beating around bush guy know ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Advanced Text Cleaning & Normalization\n",
    "# Download necessary NLTK data\n",
    "try:\n",
    "    stopwords.words(\"english\")\n",
    "except LookupError:\n",
    "    nltk.download(\"stopwords\")\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download(\"punkt\")\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "# Contraction mapping\n",
    "CONTRACTION_MAP = {\n",
    "    \"ain't\": \"is not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\", \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\", \"couldn't\": \"could not\", \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\", \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he will\", \"he'll've\": \"he will have\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\", \"how's\": \"how is\", \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\",\n",
    "    \"I'll've\": \"I will have\", \"I'm\": \"I am\", \"I've\": \"I have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\", \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\", \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\", \"needn't've\": \"need not have\", \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\",\n",
    "    \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\", \"so's\": \"so is\", \"that'd\": \"that would\", \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\",\n",
    "    \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\",\n",
    "    \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\",\n",
    "    \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\",\n",
    "    \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "    \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\", \"y'all'd\": \"you all would\", \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\", \"y'all've\": \"you all have\", \"you'd\": \"you would\", \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def advanced_clean_utt(text: str) -> str:\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Expand contractions\n",
    "    text = ' '.join([CONTRACTION_MAP.get(t, t) for t in text.split()])\n",
    "    # Remove punctuation and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Lemmatize and remove stop words\n",
    "    clean_tokens = [\n",
    "        lemmatizer.lemmatize(token) for token in tokens \n",
    "        if token not in stop_words and len(token) > 1\n",
    "    ]\n",
    "    return ' '.join(clean_tokens)\n",
    "\n",
    "# Apply cleaning function\n",
    "for df in [train_df, val_df, test_df]:\n",
    "    df['cleaned_utterance'] = df['utterance'].apply(advanced_clean_utt)\n",
    "\n",
    "print(\"Cleaned training data sample:\")\n",
    "train_df[['utterance', 'cleaned_utterance']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07837fa9",
   "metadata": {},
   "source": [
    "**handling imbalance in training set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f95bf0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training set distribution:\n",
      "emotion\n",
      "no_emotion    68524\n",
      "happiness     10513\n",
      "surprise       1455\n",
      "sadness         947\n",
      "anger           832\n",
      "disgust         273\n",
      "fear            143\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Balanced training set distribution:\n",
      "emotion\n",
      "no_emotion    10513\n",
      "happiness     10513\n",
      "surprise       1455\n",
      "sadness         947\n",
      "anger           832\n",
      "disgust         273\n",
      "fear            143\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Saved balanced training set to: data_proc/train_balanced.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Original distribution in training set\n",
    "print(\"Original training set distribution:\")\n",
    "print(train_df['emotion'].value_counts())\n",
    "\n",
    "# Separate majority and minority classes\n",
    "majority_class = train_df[train_df['emotion'] == 'no_emotion']\n",
    "minority_classes = train_df[train_df['emotion'] != 'no_emotion']\n",
    "\n",
    "# Get the size of the next largest class\n",
    "undersample_size = len(train_df[train_df['emotion'] == 'happiness'])\n",
    "\n",
    "# Undersample the majority class\n",
    "majority_undersampled = majority_class.sample(\n",
    "    n=undersample_size, \n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "# Combine with minority classes to create a balanced training set\n",
    "train_df_balanced = pd.concat([majority_undersampled, minority_classes])\n",
    "\n",
    "# Shuffle the balanced dataset\n",
    "train_df_balanced = train_df_balanced.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nBalanced training set distribution:\")\n",
    "print(train_df_balanced['emotion'].value_counts())\n",
    "\n",
    "# Save the balanced training set\n",
    "balanced_train_csv = PROC / \"train_balanced.csv\"\n",
    "train_df_balanced.to_csv(balanced_train_csv, index=False, encoding=\"utf-8\")\n",
    "print(f\"\\nSaved balanced training set to: {balanced_train_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b57947b",
   "metadata": {},
   "source": [
    "**Vectorization for model training**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a707ff63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF vectorization complete.\n",
      "X_train shape: (24676, 5000)\n",
      "X_val shape: (10268, 5000)\n",
      "X_test shape: (10024, 5000)\n",
      "\n",
      "Saved vectorizer to: data_proc/tfidf_vectorizer.joblib\n",
      "Saved data matrices to: data_proc/\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF Vectorization\n",
    "\n",
    "# Initialize the vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),      # Use unigrams and bigrams\n",
    "    max_features=5000,       # Keep top 5k features\n",
    "    sublinear_tf=True        # Apply sublinear TF scaling\n",
    ")\n",
    "\n",
    "# Fit on the balanced training data and transform all splits\n",
    "X_train = tfidf_vectorizer.fit_transform(train_df_balanced['cleaned_utterance'])\n",
    "X_val = tfidf_vectorizer.transform(val_df['cleaned_utterance'])\n",
    "X_test = tfidf_vectorizer.transform(test_df['cleaned_utterance'])\n",
    "\n",
    "# Get the labels\n",
    "y_train = train_df_balanced['emotion_id']\n",
    "y_val = val_df['emotion_id']\n",
    "y_test = test_df['emotion_id']\n",
    "\n",
    "# Save the vectorizer and the processed data\n",
    "joblib.dump(tfidf_vectorizer, PROC / 'tfidf_vectorizer.joblib')\n",
    "\n",
    "# Save the sparse matrices\n",
    "save_npz(PROC / 'X_train.npz', X_train)\n",
    "save_npz(PROC / 'X_val.npz', X_val)\n",
    "save_npz(PROC / 'X_test.npz', X_test)\n",
    "\n",
    "# Save the labels\n",
    "y_train.to_csv(PROC / 'y_train.csv', index=False, header=True)\n",
    "y_val.to_csv(PROC / 'y_val.csv', index=False, header=True)\n",
    "y_test.to_csv(PROC / 'y_test.csv', index=False, header=True)\n",
    "\n",
    "print(\"TF-IDF vectorization complete.\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"\\nSaved vectorizer to: {PROC / 'tfidf_vectorizer.joblib'}\")\n",
    "print(f\"Saved data matrices to: {PROC}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712ac72c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8b87ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LogisticRegression baseline...\n",
      "Validation Macro-F1: 0.3035\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.79      0.85      8663\n",
      "           1       0.18      0.05      0.08        80\n",
      "           2       0.67      0.09      0.15        47\n",
      "           3       0.00      0.00      0.00        12\n",
      "           4       0.33      0.72      0.45      1183\n",
      "           5       0.21      0.25      0.23       106\n",
      "           6       0.38      0.34      0.36       177\n",
      "\n",
      "    accuracy                           0.76     10268\n",
      "   macro avg       0.39      0.32      0.30     10268\n",
      "weighted avg       0.83      0.76      0.78     10268\n",
      "\n",
      "Test Macro-F1: 0.3351\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.78      0.85      8385\n",
      "           1       0.45      0.09      0.15       110\n",
      "           2       0.38      0.09      0.15        33\n",
      "           3       1.00      0.05      0.10        19\n",
      "           4       0.34      0.75      0.47      1189\n",
      "           5       0.28      0.37      0.32        97\n",
      "           6       0.36      0.27      0.31       191\n",
      "\n",
      "    accuracy                           0.76     10024\n",
      "   macro avg       0.53      0.34      0.34     10024\n",
      "weighted avg       0.83      0.76      0.78     10024\n",
      "\n",
      "Saved model and metrics to: data_proc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anshureddy/Desktop/NLP Project/nlp-team17/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Users/anshureddy/Desktop/NLP Project/nlp-team17/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/anshureddy/Desktop/NLP Project/nlp-team17/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/anshureddy/Desktop/NLP Project/nlp-team17/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/anshureddy/Desktop/NLP Project/nlp-team17/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/anshureddy/Desktop/NLP Project/nlp-team17/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/anshureddy/Desktop/NLP Project/nlp-team17/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved confusion matrix to: data_proc/confusion_matrix_test.png\n"
     ]
    }
   ],
   "source": [
    "# Baseline training: Logistic Regression on TF-IDF features\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "\n",
    "# Train classifier\n",
    "clf = LogisticRegression(\n",
    "    multi_class='multinomial',\n",
    "    solver='saga',\n",
    "    max_iter=2000,\n",
    "    #class_weight='balanced',\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "print(\"Training LogisticRegression baseline...\")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Validation metrics\n",
    "y_val_pred = clf.predict(X_val)\n",
    "val_report = classification_report(y_val, y_val_pred, output_dict=True)\n",
    "val_macro_f1 = f1_score(y_val, y_val_pred, average='macro')\n",
    "print(f\"Validation Macro-F1: {val_macro_f1:.4f}\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "# Test metrics\n",
    "y_test_pred = clf.predict(X_test)\n",
    "test_report = classification_report(y_test, y_test_pred, output_dict=True)\n",
    "test_macro_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
    "print(f\"Test Macro-F1: {test_macro_f1:.4f}\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "# Save model and metrics\n",
    "joblib.dump(clf, PROC / 'logistic_baseline.joblib')\n",
    "metrics = {\n",
    "    'validation': val_report,\n",
    "    'validation_macro_f1': float(val_macro_f1),\n",
    "    'test': test_report,\n",
    "    'test_macro_f1': float(test_macro_f1)\n",
    "}\n",
    "with open(PROC / 'baseline_metrics.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "print(f\"Saved model and metrics to: {PROC}\")\n",
    "\n",
    "# Confusion matrix plot (test)\n",
    "cm = confusion_matrix(y_test, y_test_pred, labels=sorted(EMO_ID2NAME.keys()))\n",
    "cm_norm = cm.astype('float') / (cm.sum(axis=1)[:, np.newaxis] + 1e-12)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.imshow(cm_norm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Normalized Confusion Matrix (test)')\n",
    "plt.colorbar()\n",
    "labels = [EMO_ID2NAME[i] for i in sorted(EMO_ID2NAME.keys())]\n",
    "plt.xticks(np.arange(len(labels)), labels, rotation=45, ha='right')\n",
    "plt.yticks(np.arange(len(labels)), labels)\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.tight_layout()\n",
    "plt.savefig(PROC / 'confusion_matrix_test.png', dpi=150)\n",
    "plt.close()\n",
    "print(f\"Saved confusion matrix to: {PROC / 'confusion_matrix_test.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d9dc8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`cleaned_utterance` column missing in test set — using simple clean fallback\n",
      "\n",
      "Dialogue 9542 — 16 turns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>turn_id</th>\n",
       "      <th>utterance</th>\n",
       "      <th>true_emotion</th>\n",
       "      <th>pred_emotion</th>\n",
       "      <th>pred_max_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Excuse me , sir . Can I help you ?</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>0.937777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I'd be glad to have your help . Could you make...</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>happiness</td>\n",
       "      <td>0.657833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>I'm sorry . We can not regulate the air-condit...</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>0.748632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Good idea .</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>happiness</td>\n",
       "      <td>0.959160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Your wet towel , sir .</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>0.818046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Thank you . Could I have something cold to dri...</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>happiness</td>\n",
       "      <td>0.517129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Yes . We have mineral water , orange juice , c...</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>0.865185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Orange juice , please .</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>0.868913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Would you like some ice in your drink ?</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>happiness</td>\n",
       "      <td>0.551966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Yes .</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>0.678648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>How much would you like ?</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>0.616978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>One or two cubes will be OK .</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>0.791853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>Here you are , sir .</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>0.678171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>I'm awfully sorry to have bothered you .</td>\n",
       "      <td>happiness</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.800058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>It's my pleasure .</td>\n",
       "      <td>happiness</td>\n",
       "      <td>happiness</td>\n",
       "      <td>0.934595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>Please accept this tip .</td>\n",
       "      <td>happiness</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>0.717585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    turn_id                                          utterance true_emotion  \\\n",
       "0         0                 Excuse me , sir . Can I help you ?   no_emotion   \n",
       "1         1  I'd be glad to have your help . Could you make...   no_emotion   \n",
       "2         2  I'm sorry . We can not regulate the air-condit...   no_emotion   \n",
       "3         3                                        Good idea .   no_emotion   \n",
       "4         4                             Your wet towel , sir .   no_emotion   \n",
       "5         5  Thank you . Could I have something cold to dri...   no_emotion   \n",
       "6         6  Yes . We have mineral water , orange juice , c...   no_emotion   \n",
       "7         7                            Orange juice , please .   no_emotion   \n",
       "8         8            Would you like some ice in your drink ?   no_emotion   \n",
       "9         9                                              Yes .   no_emotion   \n",
       "10       10                          How much would you like ?   no_emotion   \n",
       "11       11                      One or two cubes will be OK .   no_emotion   \n",
       "12       12                               Here you are , sir .   no_emotion   \n",
       "13       13           I'm awfully sorry to have bothered you .    happiness   \n",
       "14       14                                 It's my pleasure .    happiness   \n",
       "15       15                           Please accept this tip .    happiness   \n",
       "\n",
       "   pred_emotion  pred_max_prob  \n",
       "0    no_emotion       0.937777  \n",
       "1     happiness       0.657833  \n",
       "2    no_emotion       0.748632  \n",
       "3     happiness       0.959160  \n",
       "4    no_emotion       0.818046  \n",
       "5     happiness       0.517129  \n",
       "6    no_emotion       0.865185  \n",
       "7    no_emotion       0.868913  \n",
       "8     happiness       0.551966  \n",
       "9    no_emotion       0.678648  \n",
       "10   no_emotion       0.616978  \n",
       "11   no_emotion       0.791853  \n",
       "12   no_emotion       0.678171  \n",
       "13      sadness       0.800058  \n",
       "14    happiness       0.934595  \n",
       "15   no_emotion       0.717585  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved timeline plot: data_proc/timeline_dialog_9542.png\n",
      "\n",
      "Dialogue 5269 — 12 turns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>turn_id</th>\n",
       "      <th>utterance</th>\n",
       "      <th>true_emotion</th>\n",
       "      <th>pred_emotion</th>\n",
       "      <th>pred_max_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Do you like flowers ?</td>\n",
       "      <td>happiness</td>\n",
       "      <td>happiness</td>\n",
       "      <td>0.600869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Of course , I like .</td>\n",
       "      <td>happiness</td>\n",
       "      <td>happiness</td>\n",
       "      <td>0.658140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>What's your favorite flower ?</td>\n",
       "      <td>happiness</td>\n",
       "      <td>happiness</td>\n",
       "      <td>0.758778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Forsythia . It's also called winter jasmine wh...</td>\n",
       "      <td>happiness</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>0.513889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Spring is a lively season .</td>\n",
       "      <td>happiness</td>\n",
       "      <td>happiness</td>\n",
       "      <td>0.678546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Yes . How about you ?</td>\n",
       "      <td>happiness</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>0.678648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>I admire plum blossoms very much . It seems th...</td>\n",
       "      <td>happiness</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>0.597562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>You have a perfect taste !</td>\n",
       "      <td>happiness</td>\n",
       "      <td>happiness</td>\n",
       "      <td>0.932757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>It is the symbol of laughing at hoar frost and...</td>\n",
       "      <td>happiness</td>\n",
       "      <td>happiness</td>\n",
       "      <td>0.566749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>But it used to be ignored by many people .</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>0.832052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>The bright people will remember it forever .</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>0.462584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>Well , it's snowing . Let's enjoy the plum blo...</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>happiness</td>\n",
       "      <td>0.826033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    turn_id                                          utterance true_emotion  \\\n",
       "0         0                              Do you like flowers ?    happiness   \n",
       "1         1                               Of course , I like .    happiness   \n",
       "2         2                      What's your favorite flower ?    happiness   \n",
       "3         3  Forsythia . It's also called winter jasmine wh...    happiness   \n",
       "4         4                        Spring is a lively season .    happiness   \n",
       "5         5                              Yes . How about you ?    happiness   \n",
       "6         6  I admire plum blossoms very much . It seems th...    happiness   \n",
       "7         7                         You have a perfect taste !    happiness   \n",
       "8         8  It is the symbol of laughing at hoar frost and...    happiness   \n",
       "9         9         But it used to be ignored by many people .   no_emotion   \n",
       "10       10       The bright people will remember it forever .   no_emotion   \n",
       "11       11  Well , it's snowing . Let's enjoy the plum blo...   no_emotion   \n",
       "\n",
       "   pred_emotion  pred_max_prob  \n",
       "0     happiness       0.600869  \n",
       "1     happiness       0.658140  \n",
       "2     happiness       0.758778  \n",
       "3    no_emotion       0.513889  \n",
       "4     happiness       0.678546  \n",
       "5    no_emotion       0.678648  \n",
       "6    no_emotion       0.597562  \n",
       "7     happiness       0.932757  \n",
       "8     happiness       0.566749  \n",
       "9    no_emotion       0.832052  \n",
       "10   no_emotion       0.462584  \n",
       "11    happiness       0.826033  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved timeline plot: data_proc/timeline_dialog_5269.png\n",
      "\n",
      "Dialogue 6343 — 6 turns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>turn_id</th>\n",
       "      <th>utterance</th>\n",
       "      <th>true_emotion</th>\n",
       "      <th>pred_emotion</th>\n",
       "      <th>pred_max_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Maybe we all will be all things to all men .</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>0.760013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>How terrible !</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.408615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>But for the life , we'll be changed by this so...</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>0.655885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>I really don't want to go into the world , I f...</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.336459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Don't be silly . We have reached the age to ta...</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>0.566780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>I know .</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>0.520597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   turn_id                                          utterance true_emotion  \\\n",
       "0        0       Maybe we all will be all things to all men .   no_emotion   \n",
       "1        1                                     How terrible !   no_emotion   \n",
       "2        2  But for the life , we'll be changed by this so...   no_emotion   \n",
       "3        3  I really don't want to go into the world , I f...   no_emotion   \n",
       "4        4  Don't be silly . We have reached the age to ta...   no_emotion   \n",
       "5        5                                           I know .   no_emotion   \n",
       "\n",
       "  pred_emotion  pred_max_prob  \n",
       "0   no_emotion       0.760013  \n",
       "1      sadness       0.408615  \n",
       "2   no_emotion       0.655885  \n",
       "3      sadness       0.336459  \n",
       "4   no_emotion       0.566780  \n",
       "5   no_emotion       0.520597  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved timeline plot: data_proc/timeline_dialog_6343.png\n",
      "\n",
      "Saved sample predictions to: data_proc/sample_dialog_predictions.json\n"
     ]
    }
   ],
   "source": [
    "# Inference demo: load baseline and show predictions for a few test dialogues\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import json\n",
    "\n",
    "# Paths\n",
    "vec_path = PROC / 'tfidf_vectorizer.joblib'\n",
    "model_path = PROC / 'logistic_baseline.joblib'\n",
    "test_csv = PROC / 'test.csv'\n",
    "out_dir = PROC\n",
    "\n",
    "# Load artifacts\n",
    "vec = joblib.load(vec_path)\n",
    "clf = joblib.load(model_path)\n",
    "\n",
    "df_test = pd.read_csv(test_csv)\n",
    "\n",
    "# Ensure cleaned_utterance exists — fall back to applying basic cleaning if missing\n",
    "if 'cleaned_utterance' not in df_test.columns:\n",
    "    print('`cleaned_utterance` column missing in test set — using simple clean fallback')\n",
    "    def simple_clean(s):\n",
    "        s = str(s).lower()\n",
    "        s = re.sub(r\"[^a-zA-Z\\s]\", \"\", s)\n",
    "        s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "        return s\n",
    "    df_test['cleaned_utterance'] = df_test['utterance'].fillna('').apply(simple_clean)\n",
    "\n",
    "# Choose a few sample dialogues from the test set (if fewer dialogs exist, take all)\n",
    "unique_dialogs = df_test['dialog_id'].drop_duplicates()\n",
    "count = min(3, len(unique_dialogs))\n",
    "sample_dialogs = unique_dialogs.sample(n=count, random_state=SEED).tolist()\n",
    "\n",
    "results = {}\n",
    "for did in sample_dialogs:\n",
    "    d = df_test[df_test['dialog_id'] == did].sort_values('turn_id')\n",
    "    texts = d['cleaned_utterance'].fillna(d['utterance']).tolist()\n",
    "    X = vec.transform(texts)\n",
    "    probs = clf.predict_proba(X)\n",
    "    preds = clf.predict(X)\n",
    "    pred_ids = [int(p) for p in preds]\n",
    "    true_ids = d['emotion_id'].astype(int).tolist()\n",
    "\n",
    "    # Build a small DataFrame of results\n",
    "    row_df = pd.DataFrame({\n",
    "        'turn_id': d['turn_id'].tolist(),\n",
    "        'utterance': d['utterance'].tolist(),\n",
    "        'true_emotion': [EMO_ID2NAME.get(i,'unk') for i in true_ids],\n",
    "        'pred_emotion': [EMO_ID2NAME.get(i,'unk') for i in pred_ids],\n",
    "        'pred_max_prob': probs.max(axis=1)\n",
    "    })\n",
    "\n",
    "    results[did] = {\n",
    "        'rows': row_df,\n",
    "        'probs': probs.tolist()\n",
    "    }\n",
    "\n",
    "    print(f\"\\nDialogue {did} — {len(texts)} turns\")\n",
    "    display(row_df)\n",
    "\n",
    "    # Plot timeline: probability for top-3 predicted classes per utterance\n",
    "    top_k = 3\n",
    "    top_class_indices = np.argsort(np.mean(probs, axis=0))[-top_k:][::-1]\n",
    "    plt.figure(figsize=(8,3))\n",
    "    for idx in top_class_indices:\n",
    "        plt.plot(np.arange(len(texts)), probs[:, idx], marker='o', label=EMO_ID2NAME.get(idx))\n",
    "    plt.xticks(np.arange(len(texts)), row_df['turn_id'])\n",
    "    plt.xlabel('Turn (index)')\n",
    "    plt.ylabel('Predicted probability')\n",
    "    plt.title(f'Dialogue {did} — top {top_k} class probabilities')\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    plot_path = out_dir / f'timeline_dialog_{did}.png'\n",
    "    plt.savefig(plot_path, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"Saved timeline plot: {plot_path}\")\n",
    "\n",
    "# Save a compact JSON of sample predictions\n",
    "out_preds = {str(did): results[did]['rows'].to_dict(orient='records') for did in results}\n",
    "with open(out_dir / 'sample_dialog_predictions.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(out_preds, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print('\\nSaved sample predictions to:', out_dir / 'sample_dialog_predictions.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12bad8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GloVe file: data_proc/glove.6B.100d.txt\n",
      "Loaded splits:  82687 10268 10024\n",
      "Loading existing label encoder...\n",
      "Num labels: 7\n",
      "Vocab size (used): 17862\n",
      "Saved tokenizer to: data_proc/bilstm_tokenizer.joblib\n",
      "Sequences shapes: (82687, 50) (10268, 50) (10024, 50)\n",
      "Loading GloVe vectors (this may take a moment)...\n",
      "Loaded 400,000 glove vectors\n",
      "Built embedding matrix: (17862, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anshureddy/Desktop/NLP Project/nlp-team17/.venv/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "2025-11-17 20:09:46.057623: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3\n",
      "2025-11-17 20:09:46.057664: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-11-17 20:09:46.057679: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2025-11-17 20:09:46.057711: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-11-17 20:09:46.057726: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,786,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │     \u001b[38;5;34m1,786,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,786,200</span> (6.81 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,786,200\u001b[0m (6.81 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,786,200</span> (6.81 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,786,200\u001b[0m (6.81 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-17 20:09:47.233347: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_accuracy improved from None to 0.86005, saving model to data_proc/bilstm_glove.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2584/2584 - 88s - 34ms/step - accuracy: 0.8390 - loss: 0.5240 - val_accuracy: 0.8601 - val_loss: 0.4453\n",
      "Epoch 2/8\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.86005 to 0.86473, saving model to data_proc/bilstm_glove.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2584/2584 - 84s - 32ms/step - accuracy: 0.8491 - loss: 0.4689 - val_accuracy: 0.8647 - val_loss: 0.4289\n",
      "Epoch 3/8\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.86473 to 0.86560, saving model to data_proc/bilstm_glove.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2584/2584 - 83s - 32ms/step - accuracy: 0.8522 - loss: 0.4516 - val_accuracy: 0.8656 - val_loss: 0.4201\n",
      "Epoch 4/8\n",
      "\n",
      "Epoch 4: val_accuracy improved from 0.86560 to 0.86697, saving model to data_proc/bilstm_glove.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2584/2584 - 83s - 32ms/step - accuracy: 0.8540 - loss: 0.4378 - val_accuracy: 0.8670 - val_loss: 0.4154\n",
      "Epoch 5/8\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.86697\n",
      "2584/2584 - 83s - 32ms/step - accuracy: 0.8547 - loss: 0.4257 - val_accuracy: 0.8663 - val_loss: 0.4174\n",
      "Epoch 6/8\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.86697\n",
      "2584/2584 - 83s - 32ms/step - accuracy: 0.8570 - loss: 0.4123 - val_accuracy: 0.8659 - val_loss: 0.4170\n",
      "Epoch 7/8\n",
      "\n",
      "Epoch 7: val_accuracy did not improve from 0.86697\n",
      "2584/2584 - 84s - 32ms/step - accuracy: 0.8586 - loss: 0.4016 - val_accuracy: 0.8664 - val_loss: 0.4227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best saved model...\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step\n",
      "Test accuracy: 0.8651\n",
      "Test macro F1: 0.2645\n",
      "\n",
      "Classification report (test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       1.00      0.01      0.02       110\n",
      "     disgust       0.00      0.00      0.00        33\n",
      "        fear       0.00      0.00      0.00        19\n",
      "   happiness       0.67      0.41      0.51      1189\n",
      "  no_emotion       0.88      0.97      0.92      8385\n",
      "     sadness       0.00      0.00      0.00        97\n",
      "    surprise       0.62      0.30      0.40       191\n",
      "\n",
      "    accuracy                           0.87     10024\n",
      "   macro avg       0.45      0.24      0.26     10024\n",
      "weighted avg       0.84      0.87      0.84     10024\n",
      "\n",
      "Saved predictions to: data_proc/bilstm_glove_predictions.csv\n",
      "Artifacts saved:\n",
      "- data_proc/bilstm_glove.h5\n",
      "- data_proc/bilstm_tokenizer.joblib\n",
      "- data_proc/label_encoder.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anshureddy/Desktop/NLP Project/nlp-team17/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/anshureddy/Desktop/NLP Project/nlp-team17/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/anshureddy/Desktop/NLP Project/nlp-team17/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "# BiLSTM + GloVe baseline (end-to-end)\n",
    "import os\n",
    "import pathlib\n",
    "import joblib\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Paths & reproducibility\n",
    "SEED = 17\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "ROOT = pathlib.Path('.')\n",
    "PROC = ROOT / 'data_proc'\n",
    "PROC.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Files\n",
    "train_csv = PROC / 'train.csv'\n",
    "val_csv   = PROC / 'val.csv'\n",
    "test_csv  = PROC / 'test.csv'\n",
    "label_enc_path = PROC / 'label_encoder.pkl'\n",
    "tokenizer_path = PROC / 'bilstm_tokenizer.joblib'\n",
    "model_path = PROC / 'bilstm_glove.h5'\n",
    "preds_path = PROC / 'bilstm_glove_predictions.csv'\n",
    "\n",
    "# GloVe file path candidates\n",
    "glove_candidates = [\n",
    "    ROOT / 'glove.6B.100d.txt',\n",
    "    PROC / 'glove.6B.100d.txt',\n",
    "    pathlib.Path('/usr/local/share/glove.6B.100d.txt'),\n",
    "    pathlib.Path('glove.6B.100d.txt')\n",
    "]\n",
    "glove_path = None\n",
    "for p in glove_candidates:\n",
    "    if p.exists():\n",
    "        glove_path = p\n",
    "        break\n",
    "if glove_path is None:\n",
    "    raise FileNotFoundError(\n",
    "        'GloVe file not found. Place \"glove.6B.100d.txt\" in the project root or data_proc.\\n'\n",
    "        'You can download from: https://nlp.stanford.edu/projects/glove/'\n",
    "    )\n",
    "print(f'Using GloVe file: {glove_path}')\n",
    "\n",
    "# Load CSVs\n",
    "train = pd.read_csv(train_csv)\n",
    "val = pd.read_csv(val_csv)\n",
    "test = pd.read_csv(test_csv)\n",
    "print('Loaded splits: ', len(train), len(val), len(test))\n",
    "\n",
    "# Ensure required columns exist\n",
    "for df_name, df in [('train', train), ('val', val), ('test', test)]:\n",
    "    if 'utterance' not in df.columns or 'emotion' not in df.columns:\n",
    "        raise ValueError(f\"{df_name}.csv must contain 'utterance' and 'emotion' columns\")\n",
    "\n",
    "# Load or create LabelEncoder\n",
    "if label_enc_path.exists():\n",
    "    print('Loading existing label encoder...')\n",
    "    label_encoder = joblib.load(label_enc_path)\n",
    "else:\n",
    "    print('label_encoder.pkl not found — fitting a new LabelEncoder on training labels and saving it')\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(train['emotion'].astype(str).tolist())\n",
    "    joblib.dump(label_encoder, label_enc_path)\n",
    "    print(f'Saved label encoder to: {label_enc_path}')\n",
    "\n",
    "# Prepare integer labels\n",
    "y_train = label_encoder.transform(train['emotion'].astype(str).tolist())\n",
    "y_val   = label_encoder.transform(val['emotion'].astype(str).tolist())\n",
    "y_test  = label_encoder.transform(test['emotion'].astype(str).tolist())\n",
    "num_labels = len(label_encoder.classes_)\n",
    "print('Num labels:', num_labels)\n",
    "\n",
    "# Tokenizer + sequences\n",
    "all_train_texts = train['utterance'].astype(str).tolist()\n",
    "MAX_WORDS = 20000\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(all_train_texts)\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = min(MAX_WORDS, len(word_index)) + 1\n",
    "print('Vocab size (used):', vocab_size)\n",
    "\n",
    "# Save tokenizer\n",
    "joblib.dump(tokenizer, tokenizer_path)\n",
    "print(f'Saved tokenizer to: {tokenizer_path}')\n",
    "\n",
    "# Convert to sequences and pad\n",
    "MAXLEN = 50\n",
    "X_train = pad_sequences(tokenizer.texts_to_sequences(train['utterance'].astype(str).tolist()), maxlen=MAXLEN, padding='post', truncating='post')\n",
    "X_val   = pad_sequences(tokenizer.texts_to_sequences(val['utterance'].astype(str).tolist()), maxlen=MAXLEN, padding='post', truncating='post')\n",
    "X_test  = pad_sequences(tokenizer.texts_to_sequences(test['utterance'].astype(str).tolist()), maxlen=MAXLEN, padding='post', truncating='post')\n",
    "print('Sequences shapes:', X_train.shape, X_val.shape, X_test.shape)\n",
    "\n",
    "# Build embedding matrix from GloVe\n",
    "EMB_DIM = 100\n",
    "embeddings_index = {}\n",
    "print('Loading GloVe vectors (this may take a moment)...')\n",
    "with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.rstrip().split(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        if coefs.shape[0] == EMB_DIM:\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "print(f'Loaded {len(embeddings_index):,} glove vectors')\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, EMB_DIM), dtype='float32')\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= vocab_size:\n",
    "        continue\n",
    "    vec = embeddings_index.get(word)\n",
    "    if vec is not None:\n",
    "        embedding_matrix[i] = vec\n",
    "# Note: index 0 stays zeros (padding)\n",
    "print('Built embedding matrix:', embedding_matrix.shape)\n",
    "\n",
    "# Build model\n",
    "def build_bilstm(vocab_size, emb_dim, maxlen, embedding_matrix, num_labels):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size,\n",
    "                        output_dim=emb_dim,\n",
    "                        weights=[embedding_matrix],\n",
    "                        input_length=maxlen,\n",
    "                        trainable=False))\n",
    "    model.add(Bidirectional(LSTM(128, return_sequences=False)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(num_labels, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "model = build_bilstm(vocab_size=vocab_size, emb_dim=EMB_DIM, maxlen=MAXLEN, embedding_matrix=embedding_matrix, num_labels=num_labels)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Callbacks\n",
    "checkpoint = ModelCheckpoint(str(model_path), monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "early = EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True, mode='max')\n",
    "\n",
    "# Train\n",
    "EPOCHS = 8\n",
    "BATCH = 32\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH,\n",
    "    callbacks=[checkpoint, early],\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Load best model (checkpoint ensured)\n",
    "if os.path.exists(model_path):\n",
    "    print('Loading best saved model...')\n",
    "    model = tf.keras.models.load_model(str(model_path))\n",
    "\n",
    "# Evaluation on test set\n",
    "y_proba = model.predict(X_test, batch_size= BATCH)\n",
    "y_pred = np.argmax(y_proba, axis=1)\n",
    "\n",
    "test_acc = accuracy_score(y_test, y_pred)\n",
    "test_macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "print(f'Test accuracy: {test_acc:.4f}')\n",
    "print(f'Test macro F1: {test_macro_f1:.4f}')\n",
    "print('\\nClassification report (test):')\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "# Save predictions (with dialog_id / turn_id if present)\n",
    "out_df = test.copy()\n",
    "out_df['pred_emotion_bilstm'] = label_encoder.inverse_transform(y_pred)\n",
    "cols = ['utterance', 'emotion', 'pred_emotion_bilstm']\n",
    "for c in ['dialog_id', 'turn_id']:\n",
    "    if c in out_df.columns:\n",
    "        cols.insert(0, c)\n",
    "out_df.to_csv(preds_path, index=False, encoding='utf-8', columns=cols)\n",
    "print(f'Saved predictions to: {preds_path}')\n",
    "\n",
    "# Save tokenizer & label encoder already done; save model architecture/weights path printed\n",
    "print('Artifacts saved:')\n",
    "print('-', model_path)\n",
    "print('-', tokenizer_path)\n",
    "print('-', label_enc_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
