{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc9c9a05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T23:20:34.793271Z",
     "iopub.status.busy": "2025-11-18T23:20:34.793192Z",
     "iopub.status.idle": "2025-11-18T23:20:35.999973Z",
     "shell.execute_reply": "2025-11-18T23:20:35.999470Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv, json, re, random, pathlib\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "from scipy.sparse import save_npz, load_npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef085da4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T23:20:36.001259Z",
     "iopub.status.busy": "2025-11-18T23:20:36.001138Z",
     "iopub.status.idle": "2025-11-18T23:20:36.003306Z",
     "shell.execute_reply": "2025-11-18T23:20:36.002920Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "SEED = 17\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Paths\n",
    "ROOT = pathlib.Path(\".\")\n",
    "RAW  = ROOT / \"data_raw\"      # contains dialogues_*.txt\n",
    "PROC = ROOT / \"data_proc\"     # will be created\n",
    "PROC.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Emotion mapping (DailyDialog)\n",
    "EMO_ID2NAME = {\n",
    "    0: \"no_emotion\",\n",
    "    1: \"anger\",\n",
    "    2: \"disgust\",\n",
    "    3: \"fear\",\n",
    "    4: \"happiness\",\n",
    "    5: \"sadness\",\n",
    "    6: \"surprise\"\n",
    "}\n",
    "EMO_NAME2ID = {v:k for k,v in EMO_ID2NAME.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357a71eb",
   "metadata": {},
   "source": [
    "**Dataset Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9de2f2ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T23:20:36.004348Z",
     "iopub.status.busy": "2025-11-18T23:20:36.004283Z",
     "iopub.status.idle": "2025-11-18T23:20:36.092361Z",
     "shell.execute_reply": "2025-11-18T23:20:36.091975Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the CORRECT augmented and balanced dataset.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utterance_id</th>\n",
       "      <th>utterance</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>You're right but I'll miss him jumping on me.</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I used to play</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>It was a kind of sixth sense .</td>\n",
       "      <td>no_emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>no, looked everywhere</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Do you have sport shirts for ladies ?</td>\n",
       "      <td>no_emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   utterance_id                                      utterance     emotion\n",
       "0             0  You're right but I'll miss him jumping on me.     sadness\n",
       "1             1                                 I used to play        fear\n",
       "2             2                 It was a kind of sixth sense .  no_emotion\n",
       "3             3                          no, looked everywhere     sadness\n",
       "4             4          Do you have sport shirts for ladies ?  no_emotion"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total utterances: 109,947\n",
      "\n",
      "Emotion distribution:\n",
      "emotion\n",
      "no_emotion    33124\n",
      "happiness     33124\n",
      "sadness       18654\n",
      "fear           7928\n",
      "anger          7576\n",
      "surprise       7144\n",
      "disgust        2397\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- Load the Correct Dataset ---\n",
    "\n",
    "# Load the new, balanced, and augmented dataset created in the previous notebook\n",
    "df_all = pd.read_csv(PROC / \"dailydialog_balanced_augmented.csv\")\n",
    "\n",
    "# The 'dialog_id' column is no longer needed for splitting, but we need a unique ID for each utterance\n",
    "# to ensure a clean train/val/test split without data leakage.\n",
    "df_all = df_all.reset_index().rename(columns={'index': 'utterance_id'})\n",
    "\n",
    "print(\"Loaded the CORRECT augmented and balanced dataset.\")\n",
    "display(df_all.head())\n",
    "\n",
    "print(f\"\\nTotal utterances: {len(df_all):,}\")\n",
    "print(\"\\nEmotion distribution:\")\n",
    "print(df_all[\"emotion\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95b1561a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T23:20:36.093393Z",
     "iopub.status.busy": "2025-11-18T23:20:36.093318Z",
     "iopub.status.idle": "2025-11-18T23:20:36.103265Z",
     "shell.execute_reply": "2025-11-18T23:20:36.102888Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 'emotion_id' column.\n",
      "Emotion to ID mapping: {0: 'anger', 1: 'disgust', 2: 'fear', 3: 'happiness', 4: 'no_emotion', 5: 'sadness', 6: 'surprise'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utterance_id</th>\n",
       "      <th>utterance</th>\n",
       "      <th>emotion</th>\n",
       "      <th>emotion_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>You're right but I'll miss him jumping on me.</td>\n",
       "      <td>sadness</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I used to play</td>\n",
       "      <td>fear</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>It was a kind of sixth sense .</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>no, looked everywhere</td>\n",
       "      <td>sadness</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Do you have sport shirts for ladies ?</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   utterance_id                                      utterance     emotion  \\\n",
       "0             0  You're right but I'll miss him jumping on me.     sadness   \n",
       "1             1                                 I used to play        fear   \n",
       "2             2                 It was a kind of sixth sense .  no_emotion   \n",
       "3             3                          no, looked everywhere     sadness   \n",
       "4             4          Do you have sport shirts for ladies ?  no_emotion   \n",
       "\n",
       "   emotion_id  \n",
       "0           5  \n",
       "1           2  \n",
       "2           4  \n",
       "3           5  \n",
       "4           4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Create Numerical Labels ---\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# The models need a numerical ID for the emotion labels.\n",
    "# We'll create an 'emotion_id' column.\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder on the full set of emotion labels to ensure consistency\n",
    "df_all['emotion_id'] = label_encoder.fit_transform(df_all['emotion'])\n",
    "\n",
    "# Save the mapping from ID to emotion name for later use\n",
    "emo_id2name = {i: label for i, label in enumerate(label_encoder.classes_)}\n",
    "with open(PROC / \"emotion_label_map.json\", \"w\") as f:\n",
    "    json.dump(emo_id2name, f)\n",
    "\n",
    "print(\"Created 'emotion_id' column.\")\n",
    "print(\"Emotion to ID mapping:\", emo_id2name)\n",
    "display(df_all.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0668d4a3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872b3e46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T23:10:13.562655Z",
     "iopub.status.busy": "2025-11-18T23:10:13.562580Z",
     "iopub.status.idle": "2025-11-18T23:10:13.804053Z",
     "shell.execute_reply": "2025-11-18T23:10:13.803563Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f96c029",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T23:10:13.805205Z",
     "iopub.status.busy": "2025-11-18T23:10:13.805135Z",
     "iopub.status.idle": "2025-11-18T23:10:13.860645Z",
     "shell.execute_reply": "2025-11-18T23:10:13.860295Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total utterances: 102,979 | dialogues with length mismatch trimmed: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dialog_id</th>\n",
       "      <th>turn_id</th>\n",
       "      <th>utterance</th>\n",
       "      <th>emotion_id</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>The kitchen stinks .</td>\n",
       "      <td>2</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>I'll throw out the garbage .</td>\n",
       "      <td>0</td>\n",
       "      <td>no_emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>So Dick , how about getting some coffee for to...</td>\n",
       "      <td>4</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Coffee ? I don ’ t honestly like that kind of ...</td>\n",
       "      <td>2</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Come on , you can at least try a little , besi...</td>\n",
       "      <td>0</td>\n",
       "      <td>no_emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>What ’ s wrong with that ? Cigarette is the th...</td>\n",
       "      <td>1</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Not for me , Dick .</td>\n",
       "      <td>0</td>\n",
       "      <td>no_emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Are things still going badly with your housegu...</td>\n",
       "      <td>0</td>\n",
       "      <td>no_emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Getting worse . Now he ’ s eating me out of ho...</td>\n",
       "      <td>1</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Leo , I really think you ’ re beating around t...</td>\n",
       "      <td>0</td>\n",
       "      <td>no_emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dialog_id  turn_id                                          utterance  \\\n",
       "0          0        0                               The kitchen stinks .   \n",
       "1          0        1                       I'll throw out the garbage .   \n",
       "2          1        0  So Dick , how about getting some coffee for to...   \n",
       "3          1        1  Coffee ? I don ’ t honestly like that kind of ...   \n",
       "4          1        2  Come on , you can at least try a little , besi...   \n",
       "5          1        3  What ’ s wrong with that ? Cigarette is the th...   \n",
       "6          1        4                                Not for me , Dick .   \n",
       "7          2        0  Are things still going badly with your housegu...   \n",
       "8          2        1  Getting worse . Now he ’ s eating me out of ho...   \n",
       "9          2        2  Leo , I really think you ’ re beating around t...   \n",
       "\n",
       "   emotion_id     emotion  \n",
       "0           2     disgust  \n",
       "1           0  no_emotion  \n",
       "2           4   happiness  \n",
       "3           2     disgust  \n",
       "4           0  no_emotion  \n",
       "5           1       anger  \n",
       "6           0  no_emotion  \n",
       "7           0  no_emotion  \n",
       "8           1       anger  \n",
       "9           0  no_emotion  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac60f5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T23:10:13.861708Z",
     "iopub.status.busy": "2025-11-18T23:10:13.861630Z",
     "iopub.status.idle": "2025-11-18T23:10:13.979599Z",
     "shell.execute_reply": "2025-11-18T23:10:13.979279Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data_proc/dailydialog_utterances.csv\n",
      "Label map: data_proc/emotion_label_map.json\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732097d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T20:23:08.506511Z",
     "iopub.status.busy": "2025-11-18T20:23:08.506432Z",
     "iopub.status.idle": "2025-11-18T20:23:08.520194Z",
     "shell.execute_reply": "2025-11-18T20:23:08.519980Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogs: 13118\n",
      "Utterances: 102979\n",
      "\n",
      "Emotion distribution (counts):\n",
      "emotion\n",
      "no_emotion    85572\n",
      "happiness     12885\n",
      "surprise       1823\n",
      "sadness        1150\n",
      "anger          1022\n",
      "disgust         353\n",
      "fear            174\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Emotion distribution (proportions):\n",
      "emotion\n",
      "no_emotion     83.1%\n",
      "happiness     12.51%\n",
      "surprise       1.77%\n",
      "sadness        1.12%\n",
      "anger          0.99%\n",
      "disgust        0.34%\n",
      "fear           0.17%\n",
      "Name: proportion, dtype: object\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ec8873",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da231bb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T23:20:36.104458Z",
     "iopub.status.busy": "2025-11-18T23:20:36.104380Z",
     "iopub.status.idle": "2025-11-18T23:20:40.703556Z",
     "shell.execute_reply": "2025-11-18T23:20:40.703187Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/venusikhakolli/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val/Test Utterances: 87957 / 10995 / 10995\n",
      "\n",
      "Wrote split files to: data_proc\n"
     ]
    }
   ],
   "source": [
    "# --- Split and Clean the Data ---\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First, split into 80% train and 20% temporary (for val/test)\n",
    "train_df, temp_df = train_test_split(\n",
    "    df_all, \n",
    "    test_size=0.2, \n",
    "    random_state=SEED, \n",
    "    stratify=df_all['emotion']  # Stratify to maintain emotion distribution\n",
    ")\n",
    "\n",
    "# Split the temporary 20% into 10% validation and 10% test (50/50 split of temp_df)\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df, \n",
    "    test_size=0.5, \n",
    "    random_state=SEED, \n",
    "    stratify=temp_df['emotion'] # Stratify again\n",
    ")\n",
    "\n",
    "# --- Text Cleaning Function and Dependencies ---\n",
    "\n",
    "# Download necessary NLTK data\n",
    "try:\n",
    "    stopwords.words(\"english\")\n",
    "except LookupError:\n",
    "    nltk.download(\"stopwords\")\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download(\"punkt\")\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "# Contraction mapping\n",
    "CONTRACTION_MAP = {\n",
    "    \"ain't\": \"is not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\", \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\", \"couldn't\": \"could not\", \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\", \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he will\", \"he'll've\": \"he will have\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\", \"how's\": \"how is\", \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\",\n",
    "    \"I'll've\": \"I will have\", \"I'm\": \"I am\", \"I've\": \"I have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\", \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\", \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\", \"needn't've\": \"need not have\", \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\",\n",
    "    \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\", \"so's\": \"so is\", \"that'd\": \"that would\", \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\",\n",
    "    \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\",\n",
    "    \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\",\n",
    "    \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\",\n",
    "    \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "    \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\", \"y'all'd\": \"you all would\", \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\", \"y'all've\": \"you all have\", \"you'd\": \"you would\", \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def advanced_clean_utt(text: str) -> str:\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Expand contractions\n",
    "    text = ' '.join([CONTRACTION_MAP.get(t, t) for t in text.split()])\n",
    "    # Remove punctuation and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Lemmatize and remove stop words\n",
    "    clean_tokens = [\n",
    "        lemmatizer.lemmatize(token) for token in tokens \n",
    "        if token not in stop_words and len(token) > 1\n",
    "    ]\n",
    "    return ' '.join(clean_tokens)\n",
    "\n",
    "for df in [train_df, val_df, test_df]:\n",
    "    df['cleaned_utterance'] = df['utterance'].apply(advanced_clean_utt)\n",
    "\n",
    "# Attach split column for reference\n",
    "train_df['split'] = \"train\"\n",
    "val_df['split']   = \"val\"\n",
    "test_df['split']  = \"test\"\n",
    "\n",
    "print(f\"Train/Val/Test Utterances: {len(train_df)} / {len(val_df)} / {len(test_df)}\")\n",
    "\n",
    "# Save the split files\n",
    "train_df.to_csv(PROC/\"train.csv\", index=False, encoding=\"utf-8\")\n",
    "val_df.to_csv(PROC/\"val.csv\", index=False, encoding=\"utf-8\")\n",
    "test_df.to_csv(PROC/\"test.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"\\nWrote split files to:\", PROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d1844e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T23:08:39.071381Z",
     "iopub.status.busy": "2025-11-18T23:08:39.071304Z",
     "iopub.status.idle": "2025-11-18T23:08:43.075712Z",
     "shell.execute_reply": "2025-11-18T23:08:43.075239Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/venusikhakolli/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned training data sample:\n",
      "                                           utterance  \\\n",
      "0                               The kitchen stinks .   \n",
      "1                       I'll throw out the garbage .   \n",
      "7  Are things still going badly with your housegu...   \n",
      "8  Getting worse . Now he ’ s eating me out of ho...   \n",
      "9  Leo , I really think you ’ re beating around t...   \n",
      "\n",
      "                                   cleaned_utterance  \n",
      "0                                      kitchen stink  \n",
      "1                                  ill throw garbage  \n",
      "7                 thing still going badly houseguest  \n",
      "8  getting worse eating house home tried talking ...  \n",
      "9  leo really think beating around bush guy know ...  \n",
      "\n",
      "Updated val.csv and test.csv with cleaned utterances.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b0c7ee6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T23:20:40.704636Z",
     "iopub.status.busy": "2025-11-18T23:20:40.704564Z",
     "iopub.status.idle": "2025-11-18T23:20:41.837784Z",
     "shell.execute_reply": "2025-11-18T23:20:41.837394Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF vectorization complete.\n",
      "X_train shape: (87957, 5000)\n",
      "X_val shape: (10995, 5000)\n",
      "X_test shape: (10995, 5000)\n",
      "\n",
      "Saved vectorizer to: data_proc/tfidf_vectorizer.joblib\n",
      "Saved data matrices to: data_proc/\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF Vectorization\n",
    "\n",
    "# Initialize the vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),      # Use unigrams and bigrams\n",
    "    max_features=5000,       # Keep top 5k features\n",
    "    sublinear_tf=True        # Apply sublinear TF scaling\n",
    ")\n",
    "\n",
    "# Fit on the balanced training data and transform all splits\n",
    "X_train = tfidf_vectorizer.fit_transform(train_df['cleaned_utterance'])\n",
    "X_val = tfidf_vectorizer.transform(val_df['cleaned_utterance'])\n",
    "X_test = tfidf_vectorizer.transform(test_df['cleaned_utterance'])\n",
    "\n",
    "# Get the labels\n",
    "y_train = train_df['emotion_id']\n",
    "y_val = val_df['emotion_id']\n",
    "y_test = test_df['emotion_id']\n",
    "\n",
    "# Save the vectorizer and the processed data\n",
    "joblib.dump(tfidf_vectorizer, PROC / 'tfidf_vectorizer.joblib')\n",
    "\n",
    "# Save the sparse matrices\n",
    "save_npz(PROC / 'X_train.npz', X_train)\n",
    "save_npz(PROC / 'X_val.npz', X_val)\n",
    "save_npz(PROC / 'X_test.npz', X_test)\n",
    "\n",
    "# Save the labels\n",
    "y_train.to_csv(PROC / 'y_train.csv', index=False, header=True)\n",
    "y_val.to_csv(PROC / 'y_val.csv', index=False, header=True)\n",
    "y_test.to_csv(PROC / 'y_test.csv', index=False, header=True)\n",
    "\n",
    "print(\"TF-IDF vectorization complete.\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"\\nSaved vectorizer to: {PROC / 'tfidf_vectorizer.joblib'}\")\n",
    "print(f\"Saved data matrices to: {PROC}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd01f0c6",
   "metadata": {},
   "source": [
    "**Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50bf12fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T23:20:41.838955Z",
     "iopub.status.busy": "2025-11-18T23:20:41.838873Z",
     "iopub.status.idle": "2025-11-18T23:20:45.176754Z",
     "shell.execute_reply": "2025-11-18T23:20:45.176146Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/venusikhakolli/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned training data sample:\n",
      "                                               utterance  \\\n",
      "9782   I could believe it, Did my friend really just ...   \n",
      "70409  Oh no, would you like to talk about it? I real...   \n",
      "22370                Thanks . I really appreciate that .   \n",
      "50586  I received a lot of emails from potential clie...   \n",
      "30742  My work involves various routine bookkeeping a...   \n",
      "\n",
      "                                       cleaned_utterance  \n",
      "9782               could believe friend really say loved  \n",
      "70409             oh would like talk really hope alright  \n",
      "22370                           thanks really appreciate  \n",
      "50586  received lot email potential client answer tim...  \n",
      "30742  work involves various routine bookkeeping basi...  \n",
      "\n",
      "Updated val.csv and test.csv with cleaned utterances.\n"
     ]
    }
   ],
   "source": [
    "# Advanced Text Cleaning & Normalization\n",
    "# Download necessary NLTK data\n",
    "try:\n",
    "    stopwords.words(\"english\")\n",
    "except LookupError:\n",
    "    nltk.download(\"stopwords\")\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download(\"punkt\")\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "# Contraction mapping\n",
    "CONTRACTION_MAP = {\n",
    "    \"ain't\": \"is not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\", \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\", \"couldn't\": \"could not\", \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\", \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he will\", \"he'll've\": \"he will have\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\", \"how's\": \"how is\", \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\",\n",
    "    \"I'll've\": \"I will have\", \"I'm\": \"I am\", \"I've\": \"I have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\", \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\", \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\", \"needn't've\": \"need not have\", \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\",\n",
    "    \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\", \"so's\": \"so is\", \"that'd\": \"that would\", \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\",\n",
    "    \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\",\n",
    "    \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\",\n",
    "    \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\",\n",
    "    \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "    \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\", \"y'all'd\": \"you all would\", \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\", \"y'all've\": \"you all have\", \"you'd\": \"you would\", \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def advanced_clean_utt(text: str) -> str:\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Expand contractions\n",
    "    text = ' '.join([CONTRACTION_MAP.get(t, t) for t in text.split()])\n",
    "    # Remove punctuation and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Lemmatize and remove stop words\n",
    "    clean_tokens = [\n",
    "        lemmatizer.lemmatize(token) for token in tokens \n",
    "        if token not in stop_words and len(token) > 1\n",
    "    ]\n",
    "    return ' '.join(clean_tokens)\n",
    "\n",
    "# Apply cleaning function\n",
    "for df in [train_df, val_df, test_df]:\n",
    "    df['cleaned_utterance'] = df['utterance'].apply(advanced_clean_utt)\n",
    "\n",
    "# --- FIX: Save the updated dataframes with cleaned text ---\n",
    "val_df.to_csv(PROC/\"val.csv\", index=False, encoding=\"utf-8\")\n",
    "test_df.to_csv(PROC/\"test.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"Cleaned training data sample:\")\n",
    "print(train_df[['utterance', 'cleaned_utterance']].head())\n",
    "print(\"\\nUpdated val.csv and test.csv with cleaned utterances.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07837fa9",
   "metadata": {},
   "source": [
    "**handling imbalance in training set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f95bf0b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T23:20:45.177881Z",
     "iopub.status.busy": "2025-11-18T23:20:45.177799Z",
     "iopub.status.idle": "2025-11-18T23:20:45.350725Z",
     "shell.execute_reply": "2025-11-18T23:20:45.350415Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training set distribution:\n",
      "emotion\n",
      "happiness     26499\n",
      "no_emotion    26499\n",
      "sadness       14923\n",
      "fear           6342\n",
      "anger          6061\n",
      "surprise       5715\n",
      "disgust        1918\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Balanced training set distribution:\n",
      "emotion\n",
      "happiness     26499\n",
      "no_emotion    26499\n",
      "sadness       14923\n",
      "fear           6342\n",
      "anger          6061\n",
      "surprise       5715\n",
      "disgust        1918\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Saved balanced training set to: data_proc/train_balanced.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Original distribution in training set\n",
    "print(\"Original training set distribution:\")\n",
    "print(train_df['emotion'].value_counts())\n",
    "\n",
    "# Separate majority and minority classes\n",
    "majority_class = train_df[train_df['emotion'] == 'no_emotion']\n",
    "minority_classes = train_df[train_df['emotion'] != 'no_emotion']\n",
    "\n",
    "# Get the size of the next largest class\n",
    "undersample_size = len(train_df[train_df['emotion'] == 'happiness'])\n",
    "\n",
    "# Undersample the majority class\n",
    "majority_undersampled = majority_class.sample(\n",
    "    n=undersample_size, \n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "# Combine with minority classes to create a balanced training set\n",
    "train_df_balanced = pd.concat([majority_undersampled, minority_classes])\n",
    "\n",
    "# Shuffle the balanced dataset\n",
    "train_df_balanced = train_df_balanced.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nBalanced training set distribution:\")\n",
    "print(train_df_balanced['emotion'].value_counts())\n",
    "\n",
    "# Save the balanced training set\n",
    "balanced_train_csv = PROC / \"train_balanced.csv\"\n",
    "train_df_balanced.to_csv(balanced_train_csv, index=False, encoding=\"utf-8\")\n",
    "print(f\"\\nSaved balanced training set to: {balanced_train_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b57947b",
   "metadata": {},
   "source": [
    "**Vectorization for model training**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a707ff63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T23:20:45.352230Z",
     "iopub.status.busy": "2025-11-18T23:20:45.352144Z",
     "iopub.status.idle": "2025-11-18T23:20:46.504508Z",
     "shell.execute_reply": "2025-11-18T23:20:46.504106Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF vectorization complete.\n",
      "X_train shape: (87957, 5000)\n",
      "X_val shape: (10995, 5000)\n",
      "X_test shape: (10995, 5000)\n",
      "\n",
      "Saved vectorizer to: data_proc/tfidf_vectorizer.joblib\n",
      "Saved data matrices to: data_proc/\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF Vectorization\n",
    "\n",
    "# Initialize the vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),      # Use unigrams and bigrams\n",
    "    max_features=5000,       # Keep top 5k features\n",
    "    sublinear_tf=True        # Apply sublinear TF scaling\n",
    ")\n",
    "\n",
    "# Fit on the balanced training data and transform all splits\n",
    "X_train = tfidf_vectorizer.fit_transform(train_df_balanced['cleaned_utterance'])\n",
    "X_val = tfidf_vectorizer.transform(val_df['cleaned_utterance'])\n",
    "X_test = tfidf_vectorizer.transform(test_df['cleaned_utterance'])\n",
    "\n",
    "# Get the labels\n",
    "y_train = train_df_balanced['emotion_id']\n",
    "y_val = val_df['emotion_id']\n",
    "y_test = test_df['emotion_id']\n",
    "\n",
    "# Save the vectorizer and the processed data\n",
    "joblib.dump(tfidf_vectorizer, PROC / 'tfidf_vectorizer.joblib')\n",
    "\n",
    "# Save the sparse matrices\n",
    "save_npz(PROC / 'X_train.npz', X_train)\n",
    "save_npz(PROC / 'X_val.npz', X_val)\n",
    "save_npz(PROC / 'X_test.npz', X_test)\n",
    "\n",
    "# Save the labels\n",
    "y_train.to_csv(PROC / 'y_train.csv', index=False, header=True)\n",
    "y_val.to_csv(PROC / 'y_val.csv', index=False, header=True)\n",
    "y_test.to_csv(PROC / 'y_test.csv', index=False, header=True)\n",
    "\n",
    "print(\"TF-IDF vectorization complete.\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"\\nSaved vectorizer to: {PROC / 'tfidf_vectorizer.joblib'}\")\n",
    "print(f\"Saved data matrices to: {PROC}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712ac72c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
